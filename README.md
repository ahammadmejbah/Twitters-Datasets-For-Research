# <p align="center">Twitters Datasets For Research</p>

## <p align="center">Twitters Datasets</p>

1. <a href="https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_Veracity_Classification/6392078">PHEME dataset for Rumour Detection and Veracity Classification:</a> This dataset contains a collection of Twitter rumours and non-rumours posted during breaking news. It contains rumours related to 9 events and each of the rumours is annotated with its veracity value, either True, False or Unverified.
2. <a href="https://figshare.com/articles/Twitter_event_datasets_2012-2016_/5100460">Twitter event datasets (2012-2016):</a> data for 30 different Twitter datasets associated with real world events.
PHEME dataset of rumours and non-rumours: This dataset contains a collection of Twitter rumours and non-rumours posted during five breaking news events.
3. <a href="https://figshare.com/articles/Tweet_geolocation_5m/3168529">Tweet geolocation 5m:</a> This is a dataset with more than 5 million geolocated tweets with detailed geolocation information associated. Each geolocated tweet is associated with its fine-grained location information, collected from OpenStreetMap using the reverse geocoding feature in Nominatim.
4. <a href="http://komunitatea.elhuyar.org/tweetmt/resources/#Downloads">TweetMT: </a>A dataset for machine translation of tweets.
5. <a href="http://komunitatea.elhuyar.org/tweetlid/resources/#Downloads">TweetLID:</a> A dataset for tweet language identification, which includes 35k tweets with manually annotated language labels.
6. <a href="http://www.zubiaga.org/datasets/odptweets/">ODPtweets:</a> A large-scale Twitter dataset with nearly 25 million tweets categorized in the structure of the Open Directory Project (ODP).
7. <a href="http://komunitatea.elhuyar.org/tweet-norm/resources/#Downloads">tweet-norm_es:</a> Tweets in Spanish language, annotated for lexical normalization purposes. Created for the tweet normalization challenge at Tweet-Norm 2013.
8. <a href="http://nlp.uned.es/~damiano/datasets/TT-classification.html">Trending topics:</a> A dataset with 1,036 categorized trending topics, which we used in Real-Time Classification of Twitter Trends
Social tagging datasets
9. <a href="https://figshare.com/articles/Twitter_Death_Hoaxes_dataset/5688811">Twitter death hoaxes:</a> This is a dataset of death reports collected from Twitter between 1st January, 2012 and 31st December, 2014. It was collected by tracking the keyword 'RIP', and matching those tweets in which a name is mentioned next to RIP. Matching names were identified by using Wikidata as a database of names.
10. <a href="https://figshare.com/articles/PHEME_rumour_scheme_dataset_journalism_use_case/2068650">PHEME rumour dataset:</a> This is a dataset of conversations around rumours associated with 9 different breaking news stories, collected from Twitter. It was developed within the journalism use case of the PHEME FP7 project. Each tweet is annotated for support, certainty, and evidentiality.
11. <a href="http://www.zubiaga.org/datasets/socialbm0311/">SocialBM0311:</a> A large-scale, longitudinal social tagging dataset collected from Delicious.com. It contains the complete bookmarking activity for 2 million users from the launch of the social bookmarking website in 2003 to the end of March 2011.
12. <a href="http://www.zubiaga.org/datasets/socialodp2k9/">Social-ODP-2k9:</a> 12,616 unique URLs, with categories from the Open Directory Project (ODP/Dmoz) and a variety of social annotations (tags, notes, reviews,...) retrieved from Delicious and StumbleUpon.
13. <a href="http://www.zubiaga.org/datasets/delicioust140/">DeliciousT140:</a> 144,574 unique URLs, with social tags retrieved from Delicious.
14. <a href="http://www.zubiaga.org/datasets/wiki10+/">Wiki10+:</a> 20,764 English Wikipedia articles, with social tags retrieved from Delicious.
15. <a href="http://www.zubiaga.org/datasets/hurricane-sandy-tweets/">Hurricane Sandy tweets:</a> Nearly 15 million tweets posted on Twitter while Hurricane Sandy was hitting the East Coast of the United States, as well as in the aftermath.

#### <a href="http://www.zubiaga.org"> <p align="center">Credit: Arkaitz Zubiaga</p> </a>
